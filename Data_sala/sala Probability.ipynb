{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "X, y = make_blobs(n_samples=1000, random_state=42, cluster_std=5.0)\n",
    "X_train, y_train = X[:600], y[:600]\n",
    "X_valid, y_valid = X[600:800], y[600:800]\n",
    "X_train_valid, y_train_valid = X[:800], y[:800]\n",
    "X_test, y_test = X[800:], y[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.46318783, -8.17463446],\n",
       "       [-3.16915033, -5.38364669],\n",
       "       [ 7.4315134 ,  2.35319664],\n",
       "       ...,\n",
       "       [ 5.16675637, -4.69695779],\n",
       "       [ 4.8802726 ,  6.42293504],\n",
       "       [ 1.87876361,  5.13782877]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and organize data. Use only FSR data\n",
    "\n",
    "# Import and organize data\n",
    "\n",
    "filepath = \"/Users/salatiemann/Documents/UC Berkeley/Year 3 Sem 2 (Spring 2020)/Tensegrity-HRI/Data_sala/test\"\n",
    "test_numbers = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20',\n",
    "      '21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37','38','39','40',\n",
    "      '41','42','43','44','45','46','47','48','49','50','51','52','53','54','55','56','57','58','59','60',\n",
    "      '61','62','63','64','65','66','67','68','69','70','71','72']\n",
    "data = []\n",
    "for num in test_numbers:\n",
    "    filename = filepath + num + '.csv'\n",
    "    d = pd.read_csv(filename, names = [\"time\", \"IND\", \"FSR_1\", \"FSR_2\", \"FSR_3\", \"FSR_4\", \"FSR_5\", \"FSR_6\", \"FSR_7\", \n",
    "                                   \"FSR_8\", \"FSR_9\", \"FSR_10\", \"FSR_11\", \"FSR_12\",\"ACC_X\", \"ACC_Y\", \"ACC_Z\"])\n",
    "    d1 = d.drop(columns = [\"IND\", \"time\"])\n",
    "    d1 = d1.drop(columns = [\"ACC_X\", \"ACC_Y\", \"ACC_Z\"])\n",
    "    d1 = d1.to_numpy()\n",
    "    d1 = d1[0:7000]\n",
    "    data.append(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 7000, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0,0,0,3,1,1,1,1,1,1,3,0,1,1,1,1,1,1,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,1,1,1,1,0,3,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "         1,2,3,3,3,3,3,2,2,2,0,0,1,1,2,2,3,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES\n",
    "\n",
    "# This feature returns the percent of data points that are below 75% of the max\n",
    "\n",
    "def three_quarters(column,percent):\n",
    "    max = np.max(column)\n",
    "    three_fourths = percent*max\n",
    "    e = []\n",
    "    j=0\n",
    "    for i in range(len(column)):\n",
    "        if column[i] <= .75*np.max(column):\n",
    "            j+=1\n",
    "    return j/len(column)\n",
    "\n",
    "# This feature takes the max of each column, takes the data point [change_factor] ms after that, and then normalizes \n",
    "# the difference\n",
    "\n",
    "def change_in_max(column, change_factor):\n",
    "    max_index = np.argmax(column)\n",
    "    minus_change_index = max_index - change_factor\n",
    "    if minus_change_index > 0:\n",
    "        minus_change_value = column[minus_change_index]\n",
    "    else:\n",
    "        minus_change_index = max_index + change_factor\n",
    "        minus_change_value = column[minus_change_index]\n",
    "    diff = np.max(column) - minus_change_value\n",
    "    normalized = diff/np.std(column)\n",
    "    return normalized\n",
    "\n",
    "# This function takes all 72 data points of size (7001,16) and puts them into a feature-engineered array.\n",
    "# Desired shape: 72 data points of size (6,16) --> (72,6,16)\n",
    "\n",
    "def features(array, three_quarters_percent, change_in_max_change_factor):\n",
    "    len_dataset, _, number_of_columns = np.shape(array)\n",
    "    number_of_features = 6\n",
    "    condensed_data = np.zeros((len_dataset,number_of_features,number_of_columns))\n",
    "    for j in range(72):\n",
    "        data_point = array[j]\n",
    "        number_of_columns = len(data_point[0,:])\n",
    "        D = np.zeros((number_of_features,number_of_columns),dtype=np.float64)\n",
    "        for i in range(number_of_columns):\n",
    "            column = data_point[:,i]\n",
    "            D[0,i] = np.mean(column)\n",
    "            D[1,i] = np.var(column)\n",
    "            D[2,i] = np.max(column)-np.min(column)\n",
    "            D[3,i] = three_quarters(column,three_quarters_percent)\n",
    "            D[4,i] = change_in_max(column,change_in_max_change_factor)\n",
    "            D[5,i] = np.sum(column)/np.max(column) #or np.std(column). MAX WORKS THE BEST! went from 80% or so to 100%\n",
    "        condensed_data[j]=D\n",
    "        if np.mod(j,6) == 0:\n",
    "            print(j, end=\" \")\n",
    "    return condensed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condense data into averages over a specified number of data points\n",
    "\n",
    "def condense(array, condense_factor):\n",
    "    len_dataset, _, number_of_columns = np.shape(array)\n",
    "    length = len(array[0])\n",
    "    condensed_length = int((length/condense_factor))\n",
    "    condensed_data = np.zeros((len_dataset,condensed_length,number_of_columns))\n",
    "    for j in range(len_dataset):\n",
    "        data_point = array[j]\n",
    "        for k in range(number_of_columns):\n",
    "            column = data_point[:,k]\n",
    "            for i in range(condensed_length):\n",
    "                condensed_data[j][i,k] = np.mean(column[i*condense_factor:(i*condense_factor+condense_factor)])\n",
    "    return condensed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the max of all features from the 16 datapoints. Output is 72 sets of 1 by 6 data, where each of the 6 points\n",
    "# is the max of each feature from the set. \n",
    "\n",
    "def everything_max(array):\n",
    "    len_dataset, num_features, num_datapoints = np.shape(array)\n",
    "    maxed_data = np.zeros((len_dataset, 1, num_features))\n",
    "    for i in range(len_dataset):\n",
    "        datapoint = array[i]\n",
    "        D = np.zeros((1, num_features))\n",
    "        for j in range(num_features):\n",
    "            D[0,j] = max(datapoint[:,j])\n",
    "        maxed_data[i] = D\n",
    "    return maxed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6 12 18 24 30 36 42 48 54 60 66 "
     ]
    }
   ],
   "source": [
    "# Normalize and feature-ify the data\n",
    "normalized_data = data/np.max(data)\n",
    "normalized_featured_data = features(normalized_data,.75,50)\n",
    "np.shape(normalized_featured_data)\n",
    "\n",
    "# Make dataset that only has max of each feature\n",
    "maxed_data = everything_max(normalized_featured_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 1, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn 3D array into 2D array\n",
    "np.shape(maxed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 3D array into 2D array\n",
    "\n",
    "def two_d(maxed_data):\n",
    "    len_data, _, num_points = np.shape(maxed_data)\n",
    "    data = np.zeros((len_data, num_points))\n",
    "    for i in range(len(maxed_data)):\n",
    "        data[i] = maxed_data[i][0]\n",
    "    return data\n",
    "\n",
    "x = two_d(maxed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 6)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
