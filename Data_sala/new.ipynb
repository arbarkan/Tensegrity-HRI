{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from matplotlib.pyplot import clf\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "# improve progress bar display\n",
    "import tqdm\n",
    "import tqdm.auto\n",
    "tqdm.tqdm = tqdm.auto.tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and organize data\n",
    "\n",
    "filepath = \"/Users/salatiemann/Downloads/Data copy/test\"\n",
    "test_numbers = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20',\n",
    "      '21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37','38','39','40',\n",
    "      '41','42','43','44','45','46','47','48','49','50','51','52','53','54','55','56','57','58','59','60',\n",
    "      '61','62','63','64','65','66','67','68','69','70','71','72']\n",
    "data = []\n",
    "for num in test_numbers:\n",
    "    filename = filepath + num + '.csv'\n",
    "    d = pd.read_csv(filename, names = [\"IND\", \"time\", \"FSR_1\", \"FSR_2\", \"FSR_3\", \"FSR_4\", \"FSR_5\", \"FSR_6\", \"FSR_7\", \n",
    "                                   \"FSR_8\", \"FSR_9\", \"FSR_10\", \"FSR_11\", \"FSR_12\",\"ACC_X\", \"ACC_Y\", \"ACC_Z\"])\n",
    "    d = d.drop(columns = [\"IND\"])\n",
    "    d = d.to_numpy()\n",
    "    data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 7001, 16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0,0,0,3,1,1,1,1,1,1,3,0,1,1,1,1,1,1,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,1,1,1,1,0,3,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "         1,2,3,3,3,3,3,2,2,2,0,0,1,1,2,2,3,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(labels)\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 7001, 16)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin feature engineering\n",
    "\n",
    "def three_quarters(column):\n",
    "    max = np.max(column)\n",
    "    three_fourths = .75*max\n",
    "    e = []\n",
    "    j=0\n",
    "    for i in range(len(column)):\n",
    "        if column[i] <= .75*np.max(column):\n",
    "            j+=1\n",
    "    return j/len(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feature takes the max of each column, takes the data point 10ms after that, and then normalizes the difference\n",
    "\n",
    "def change_in_max(column):\n",
    "    max_index = np.argmax(column)\n",
    "    plus_ten_index = max_index+2\n",
    "    if plus_ten_index <= 7001:\n",
    "        plus_ten_value = column[plus_ten_index]\n",
    "    else:\n",
    "        plus_ten_index = max_index-2\n",
    "        plus_ten_value = column[plus_ten_index]\n",
    "    diff = np.max(column) - plus_ten_value\n",
    "    #normalized = diff/np.max(column)\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_in_max(data[71][:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes all 72 data points of size (7001,16) and puts them into a feature-engineered array.\n",
    "# Desired shape: 72 data points of size (6,16) --> (72,6,16)\n",
    "\n",
    "def condense(array):\n",
    "    len_dataset, _, number_of_columns = np.shape(array)\n",
    "    number_of_features = 5\n",
    "    condensed_data = np.zeros((len_dataset,number_of_features,number_of_columns))\n",
    "    for j in range(72):\n",
    "        data_point = array[j]\n",
    "        number_of_columns = len(data_point[0,:])\n",
    "        D = np.zeros((number_of_features,number_of_columns),dtype=np.float64)\n",
    "        for i in range(number_of_columns):\n",
    "            column = data_point[:,i]\n",
    "            D[0,i] = np.mean(column)\n",
    "            D[1,i] = np.var(column)\n",
    "            D[2,i] = np.max(column)\n",
    "            #D[3,i] = np.std(column)\n",
    "            D[3,i] = three_quarters(column) # feature: percent of data points that are below 75% of the max\n",
    "            D[4,i] = change_in_max(column)\n",
    "        condensed_data[j]=D\n",
    "        print(j, end=\" \")\n",
    "    return condensed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 "
     ]
    },
    {
     "data": {
      "text/plain": [
       "(72, 5, 16)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featured_data = condense(data)\n",
    "featured_data\n",
    "np.shape(featured_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples, nx, ny = np.shape(featured_data)\n",
    "reformated_data = np.reshape(featured_data,(nsamples, nx*ny))\n",
    "\n",
    "train = reformated_data[0:64]\n",
    "test = reformated_data[64:72]\n",
    "train_labels = labels[0:64]\n",
    "test_labels = labels[64:72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels = [0 0 1 1 1 1 1 3]\n",
      "Correct labels =   [0 0 1 1 2 2 3 3]\n"
     ]
    }
   ],
   "source": [
    "[X_train, X_test, y_train, y_test] = [train, test, train_labels, test_labels]\n",
    "model = GaussianNB().fit(train, train_labels)\n",
    "print(\"Predicted labels = \" + str(model.predict(test)))\n",
    "print(\"Correct labels =   \" + str(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "a = model.predict(test)\n",
    "for i in range(len(a)):\n",
    "    if a[i] == test_labels[i]:\n",
    "        total += 1\n",
    "percent_correct = total/len(a)\n",
    "percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.predict(X_test)\n",
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape([train, test, train_labels, test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(reformated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = reformated_data[0:62]\n",
    "np.shape(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = labels[0:62]\n",
    "test_labels = labels[62:72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = (train, train_labels)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((features, labels)).repeat().batch(12)\n",
    "\n",
    "features, labels = (test, test_labels)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((features, labels)).repeat().batch(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = train_dataset.__iter__()\n",
    "test_iter = test_dataset.__iter__()\n",
    "\n",
    "train_x, train_y = train_iter.get_next()\n",
    "test_x, test_y = test_iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test_dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_features_vector(features, labels):\n",
    "    #Pack the features into a single array.\n",
    "    features = tf.stack(list(features, axis=1))\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(pack_features_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make a simple model\n",
    "# net = tf.keras.layers.Dense(train_x, 8) # pass the first value from iter.get_next() as input\n",
    "# net = tf.keras.layers.dense(net, 8)\n",
    "# prediction = tf.layers.dense(net, 1)\n",
    "# loss = tf.losses.mean_squared_error(prediction, train_y) # pass the second value from iter.get_net() as label\n",
    "# train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l0 = tf.keras.layers.Dense(units=1, input_shape=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(units = 112016, input_shape=(None, 7001, 16)),\n",
    "    #tf.keras.layers.Dense(units=112016, input_shape=(None, 7001, 16), activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 12\n",
    "# train = train.repeat().shuffle(62).batch(BATCH_SIZE)\n",
    "# test = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train, epochs=1, steps_per_epoch=math.ceil(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
