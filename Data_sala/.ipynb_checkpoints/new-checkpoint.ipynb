{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from matplotlib.pyplot import clf\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "# improve progress bar display\n",
    "import tqdm\n",
    "import tqdm.auto\n",
    "tqdm.tqdm = tqdm.auto.tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and organize data\n",
    "\n",
    "filepath = \"/Users/salatiemann/Documents/UC Berkeley/Year 3 Sem 2 (Spring 2020)/Tensegrity-HRI/Data_sala/test\"\n",
    "test_numbers = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20',\n",
    "      '21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37','38','39','40',\n",
    "      '41','42','43','44','45','46','47','48','49','50','51','52','53','54','55','56','57','58','59','60',\n",
    "      '61','62','63','64','65','66','67','68','69','70','71','72']\n",
    "data = []\n",
    "data_no_acc = []\n",
    "for num in test_numbers:\n",
    "    filename = filepath + num + '.csv'\n",
    "    d = pd.read_csv(filename, names = [\"time\", \"IND\", \"FSR_1\", \"FSR_2\", \"FSR_3\", \"FSR_4\", \"FSR_5\", \"FSR_6\", \"FSR_7\", \n",
    "                                   \"FSR_8\", \"FSR_9\", \"FSR_10\", \"FSR_11\", \"FSR_12\",\"ACC_X\", \"ACC_Y\", \"ACC_Z\"])\n",
    "    d1 = d.drop(columns = [\"IND\", \"time\"])\n",
    "    d2 = d1.drop(columns = [\"ACC_X\", \"ACC_Y\", \"ACC_Z\"])\n",
    "    d1 = d1.to_numpy()\n",
    "    d1 = d1[0:7000]\n",
    "    data.append(d1)\n",
    "    d2 = d2.to_numpy()\n",
    "    d2 = d2[0:7000]\n",
    "    data_no_acc.append(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 7000, 12)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data_no_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0,0,0,3,1,1,1,1,1,1,3,0,1,1,1,1,1,1,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,1,1,1,1,0,3,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "         1,2,3,3,3,3,3,2,2,2,0,0,1,1,2,2,3,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(labels)\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 7000, 15)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feature returns the percent of data points that are below 75% of the max\n",
    "\n",
    "def three_quarters(column):\n",
    "    max = np.max(column)\n",
    "    three_fourths = .75*max\n",
    "    e = []\n",
    "    j=0\n",
    "    for i in range(len(column)):\n",
    "        if column[i] <= .75*np.max(column):\n",
    "            j+=1\n",
    "    return j/len(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feature takes the max of each column, takes the data point 10ms after that, and then normalizes the difference\n",
    "\n",
    "def change_in_max(column, change_factor):\n",
    "    max_index = np.argmax(column)\n",
    "    plus_change_index = max_index + change_factor\n",
    "    if plus_change_index < len(column):\n",
    "        plus_change_value = column[plus_change_index]\n",
    "    else:\n",
    "        plus_change_index = max_index - change_factor\n",
    "        plus_change_value = column[plus_change_index]\n",
    "    diff = np.max(column) - plus_change_value\n",
    "    normalized = diff/np.std(column)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes all 72 data points of size (7001,16) and puts them into a feature-engineered array.\n",
    "# Desired shape: 72 data points of size (6,16) --> (72,6,16)\n",
    "\n",
    "def features(array):\n",
    "    len_dataset, _, number_of_columns = np.shape(array)\n",
    "    number_of_features = 6\n",
    "    condensed_data = np.zeros((len_dataset,number_of_features,number_of_columns))\n",
    "    for j in range(72):\n",
    "        data_point = array[j]\n",
    "        number_of_columns = len(data_point[0,:])\n",
    "        D = np.zeros((number_of_features,number_of_columns),dtype=np.float64)\n",
    "        for i in range(number_of_columns):\n",
    "            column = data_point[:,i]\n",
    "            D[0,i] = np.mean(column)\n",
    "            D[1,i] = np.var(column)\n",
    "            D[2,i] = np.max(column)-np.min(column)\n",
    "            D[3,i] = three_quarters(column)\n",
    "            D[4,i] = change_in_max(column,5)\n",
    "            D[5,i] = np.sum(column)/np.max(column) #or np.std(column). MAX WORKS THE BEST! went from 80% or so to 100%\n",
    "        condensed_data[j]=D\n",
    "        if np.mod(j,6) == 0:\n",
    "            print(j, end=\" \")\n",
    "    return condensed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-ify the data\n",
    "\n",
    "featured_data = features(data)\n",
    "np.shape(featured_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(72, 1400, 15)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Condense the data (on non-feature-ified data)\n",
    "\n",
    "condensed_data = condense(data)\n",
    "np.shape(condensed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the max of all features from the 16 datapoints. Output is 72 sets of 1 by 6 data, where each of the 6 points\n",
    "# is the max of each feature from the set. \n",
    "\n",
    "def everything_max(array):\n",
    "    len_dataset, num_features, num_datapoints = np.shape(array)\n",
    "    maxed_data = np.zeros((len_dataset, 1, num_features))\n",
    "    for i in range(len_dataset):\n",
    "        datapoint = array[i]\n",
    "        D = np.zeros((1, num_features))\n",
    "        for j in range(num_features):\n",
    "            D[0,j] = max(datapoint[:,j])\n",
    "        maxed_data[i] = D\n",
    "    return maxed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condense data into averages over a specified number of data points\n",
    "\n",
    "def condense(array):\n",
    "    len_dataset, _, number_of_columns = np.shape(array)\n",
    "    length = len(array[0])\n",
    "    condense_factor = 5\n",
    "    condensed_length = int((length/condense_factor))\n",
    "    condensed_data = np.zeros((len_dataset,condensed_length,number_of_columns))\n",
    "    for j in range(len_dataset):\n",
    "        data_point = array[j]\n",
    "        for k in range(number_of_columns):\n",
    "            column = data_point[:,k]\n",
    "            for i in range(condensed_length):\n",
    "                condensed_data[j][i,k] = np.mean(column[i*condense_factor:(i*condense_factor+condense_factor)])\n",
    "            print(\".\", end=\"\")\n",
    "    return condensed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make model\n",
    "\n",
    "def make_model(dataset,num_train):\n",
    "    nsamples, nx, ny = np.shape(dataset)\n",
    "    reformated_data = np.reshape(dataset,(nsamples,nx*ny))\n",
    "    \n",
    "    train = reformated_data[0:num_train]\n",
    "    test = reformated_data[num_train:nsamples]\n",
    "    train_labels = labels[0:num_train]\n",
    "    test_labels = labels[num_train:nsamples]\n",
    "    \n",
    "    # Return percent of correct labels\n",
    "    def percent_correct(test_dataset, test_labels):\n",
    "        total = 0\n",
    "        a = model.predict(test_dataset)\n",
    "        for i in range(len(a)):\n",
    "            if a[i] == test_labels[i]:\n",
    "                total += 1\n",
    "        percent_correct = total/len(a)*100\n",
    "        print('Percent correct: {}%'.format(percent_correct))\n",
    "    \n",
    "    model = GaussianNB().fit(train, train_labels)\n",
    "    print(\"Predicted labels = \" + str(model.predict(test)))\n",
    "    print(\"Correct labels =   \" + str(test_labels))\n",
    "    percent_correct(test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels = [1 0 1 1 2 2 1 3]\n",
      "Correct labels =   [0 0 1 1 2 2 3 3]\n",
      "Percent correct: 75.0%\n"
     ]
    }
   ],
   "source": [
    "# Train model on featured data (dataset size (72,6,15))\n",
    "\n",
    "make_model(featured_data,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels = [0 3 1 1 2 2 2 3]\n",
      "Correct labels =   [0 0 1 1 2 2 3 3]\n",
      "Percent correct: 75.0%\n"
     ]
    }
   ],
   "source": [
    "# Train model on condensed data (averages of data over a specified number of data points)\n",
    "# No feature engineering\n",
    "\n",
    "make_model(condensed_data,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels = [0 3 1 1 2 2 2 3]\n",
      "Correct labels =   [0 0 1 1 2 2 3 3]\n",
      "Percent correct: 75.0%\n"
     ]
    }
   ],
   "source": [
    "# Train model on full set of data with no feature engineering\n",
    "\n",
    "make_model(data,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 1, 6)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max the features of the data\n",
    "\n",
    "maxed_featured_data = everything_max(featured_data)\n",
    "np.shape(maxed_featured_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels = [0 2 2 2 0 0 1 1 2 2 1 3]\n",
      "Correct labels =   [3 2 2 2 0 0 1 1 2 2 3 3]\n",
      "Percent correct: 83.33333333333334%\n"
     ]
    }
   ],
   "source": [
    "# Train model on max-feature-engineered data. This is eeeeeasily the best model holy crap\n",
    "\n",
    "make_model(maxed_featured_data,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 ................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"
     ]
    }
   ],
   "source": [
    "# Try models with data without accelerometer data. \n",
    "\n",
    "data_no_acc\n",
    "featured_no_acc = features(data_no_acc)\n",
    "condensed_no_acc = condense(data_no_acc)\n",
    "max_featured_no_acc = everything_max(featured_no_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels = [0 2 2 2 0 0 1 1 2 2 1 3]\n",
      "Correct labels =   [3 2 2 2 0 0 1 1 2 2 3 3]\n",
      "Percent correct: 83.33333333333334%\n"
     ]
    }
   ],
   "source": [
    "# Train model on data without accelerometer data. I tried this and there was no change, so might as well not include\n",
    "# the accelerometer data unless I can think of features that differentiate between accelerometer data\n",
    "# Pretty sure having the accelerometer data makes no difference\n",
    "\n",
    "make_model(max_featured_no_acc,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best change factor for change in max function\n",
    "\n",
    "for i in range(10):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best three_quarters value. .6 --> .8?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is some Tensorflow practice stuff that I'm not currently working on but don't want to lose\n",
    "\n",
    "batch_size = 12\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = (train, train_labels)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((features, labels)).repeat().batch(12)\n",
    "\n",
    "features, labels = (test, test_labels)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((features, labels)).repeat().batch(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = train_dataset.__iter__()\n",
    "test_iter = test_dataset.__iter__()\n",
    "\n",
    "train_x, train_y = train_iter.get_next()\n",
    "test_x, test_y = test_iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test_dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_features_vector(features, labels):\n",
    "    #Pack the features into a single array.\n",
    "    features = tf.stack(list(features, axis=1))\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(pack_features_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make a simple model\n",
    "# net = tf.keras.layers.Dense(train_x, 8) # pass the first value from iter.get_next() as input\n",
    "# net = tf.keras.layers.dense(net, 8)\n",
    "# prediction = tf.layers.dense(net, 1)\n",
    "# loss = tf.losses.mean_squared_error(prediction, train_y) # pass the second value from iter.get_net() as label\n",
    "# train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l0 = tf.keras.layers.Dense(units=1, input_shape=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(units = 112016, input_shape=(None, 7001, 16)),\n",
    "    #tf.keras.layers.Dense(units=112016, input_shape=(None, 7001, 16), activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 12\n",
    "# train = train.repeat().shuffle(62).batch(BATCH_SIZE)\n",
    "# test = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train, epochs=1, steps_per_epoch=math.ceil(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
