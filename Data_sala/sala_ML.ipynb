{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import time\n",
    "\n",
    "# improve progress bar display\n",
    "import tqdm\n",
    "import tqdm.auto\n",
    "tqdm.tqdm = tqdm.auto.tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and organize data\n",
    "\n",
    "filepath = \"/Users/salatiemann/Documents/UC Berkeley/Year 3 Sem 2 (Spring 2020)/Tensegrity-HRI/Data_sala/test\"\n",
    "test_numbers = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20',\n",
    "      '21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37','38','39','40',\n",
    "      '41','42','43','44','45','46','47','48','49','50','51','52','53','54','55','56','57','58','59','60',\n",
    "      '61','62','63','64','65','66','67','68','69','70','71','72']\n",
    "data = []\n",
    "data_no_acc = []\n",
    "for num in test_numbers:\n",
    "    filename = filepath + num + '.csv'\n",
    "    d = pd.read_csv(filename, names = [\"time\", \"IND\", \"FSR_1\", \"FSR_2\", \"FSR_3\", \"FSR_4\", \"FSR_5\", \"FSR_6\", \"FSR_7\", \n",
    "                                   \"FSR_8\", \"FSR_9\", \"FSR_10\", \"FSR_11\", \"FSR_12\",\"ACC_X\", \"ACC_Y\", \"ACC_Z\"])\n",
    "    d1 = d.drop(columns = [\"IND\", \"time\"])\n",
    "    d2 = d1.drop(columns = [\"ACC_X\", \"ACC_Y\", \"ACC_Z\"])\n",
    "    d1 = d1.to_numpy()\n",
    "    d1 = d1[0:7000]\n",
    "    data.append(d1)\n",
    "    d2 = d2.to_numpy()\n",
    "    d2 = d2[0:7000]\n",
    "    data_no_acc.append(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0,0,0,3,1,1,1,1,1,1,3,0,1,1,1,1,1,1,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,1,1,1,1,0,3,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "         1,2,3,3,3,3,3,2,2,2,0,0,1,1,2,2,3,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feature returns the percent of data points that are below 75% of the max\n",
    "\n",
    "def three_quarters(column,percent):\n",
    "    max = np.max(column)\n",
    "    three_fourths = percent*max\n",
    "    e = []\n",
    "    j=0\n",
    "    for i in range(len(column)):\n",
    "        if column[i] <= .75*np.max(column):\n",
    "            j+=1\n",
    "    return j/len(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feature takes the max of each column, takes the data point [change_factor] ms after that, and then normalizes \n",
    "# the difference\n",
    "\n",
    "def change_in_max(column, change_factor):\n",
    "    max_index = np.argmax(column)\n",
    "    minus_change_index = max_index - change_factor\n",
    "    if minus_change_index > 0:\n",
    "        minus_change_value = column[minus_change_index]\n",
    "    else:\n",
    "        minus_change_index = max_index + change_factor\n",
    "        minus_change_value = column[minus_change_index]\n",
    "    diff = np.max(column) - minus_change_value\n",
    "    normalized = diff/np.std(column)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes all 72 data points of size (7001,16) and puts them into a feature-engineered array.\n",
    "# Desired shape: 72 data points of size (6,16) --> (72,6,16)\n",
    "\n",
    "def features(array, three_quarters_percent, change_in_max_change_factor):\n",
    "    len_dataset, _, number_of_columns = np.shape(array)\n",
    "    number_of_features = 6\n",
    "    condensed_data = np.zeros((len_dataset,number_of_features,number_of_columns))\n",
    "    for j in range(72):\n",
    "        data_point = array[j]\n",
    "        number_of_columns = len(data_point[0,:])\n",
    "        D = np.zeros((number_of_features,number_of_columns),dtype=np.float64)\n",
    "        for i in range(number_of_columns):\n",
    "            column = data_point[:,i]\n",
    "            D[0,i] = np.mean(column)\n",
    "            D[1,i] = np.var(column)\n",
    "            D[2,i] = np.max(column)-np.min(column)\n",
    "            D[3,i] = three_quarters(column,three_quarters_percent)\n",
    "            D[4,i] = change_in_max(column,change_in_max_change_factor)\n",
    "            D[5,i] = np.sum(column)/np.max(column) #or np.std(column). MAX WORKS THE BEST! went from 80% or so to 100%\n",
    "        condensed_data[j]=D\n",
    "        if np.mod(j,6) == 0:\n",
    "            print(j, end=\" \")\n",
    "    return condensed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condense data into averages over a specified number of data points\n",
    "\n",
    "def condense(array, condense_factor):\n",
    "    len_dataset, _, number_of_columns = np.shape(array)\n",
    "    length = len(array[0])\n",
    "    condensed_length = int((length/condense_factor))\n",
    "    condensed_data = np.zeros((len_dataset,condensed_length,number_of_columns))\n",
    "    for j in range(len_dataset):\n",
    "        data_point = array[j]\n",
    "        for k in range(number_of_columns):\n",
    "            column = data_point[:,k]\n",
    "            for i in range(condensed_length):\n",
    "                condensed_data[j][i,k] = np.mean(column[i*condense_factor:(i*condense_factor+condense_factor)])\n",
    "    return condensed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the max of all features from the 16 datapoints. Output is 72 sets of 1 by 6 data, where each of the 6 points\n",
    "# is the max of each feature from the set. \n",
    "\n",
    "def everything_max(array):\n",
    "    len_dataset, num_features, num_datapoints = np.shape(array)\n",
    "    maxed_data = np.zeros((len_dataset, 1, num_features))\n",
    "    for i in range(len_dataset):\n",
    "        datapoint = array[i]\n",
    "        D = np.zeros((1, num_features))\n",
    "        for j in range(num_features):\n",
    "            D[0,j] = max(datapoint[:,j])\n",
    "        maxed_data[i] = D\n",
    "    return maxed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make model\n",
    "\n",
    "def make_model(dataset,num_train):\n",
    "    nsamples, nx, ny = np.shape(dataset)\n",
    "    reformated_data = np.reshape(dataset,(nsamples,nx*ny))\n",
    "    \n",
    "    train = reformated_data[0:num_train]\n",
    "    test = reformated_data[num_train:nsamples]\n",
    "    train_labels = labels[0:num_train]\n",
    "    test_labels = labels[num_train:nsamples]\n",
    "    \n",
    "    # Return percent of correct labels\n",
    "    def percent_correct(test_dataset, test_labels):\n",
    "        total = 0\n",
    "        a = model.predict(test_dataset)\n",
    "        for i in range(len(a)):\n",
    "            if a[i] == test_labels[i]:\n",
    "                total += 1\n",
    "        correct = total/len(a)*100\n",
    "        return correct\n",
    "    \n",
    "    model = GaussianNB().fit(train, train_labels)\n",
    "    print(\"Predicted labels = \" + str(model.predict(test)))\n",
    "    print(\"Correct labels =   \" + str(np.asarray(test_labels)))\n",
    "    return percent_correct(test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6 12 18 24 30 36 42 48 54 60 66 0 6 12 18 24 30 36 42 48 54 60 66 "
     ]
    }
   ],
   "source": [
    "# Normalize and feature-ify the data\n",
    "normalized_data = data/np.max(data)\n",
    "normalized_featured_data = features(normalized_data,.75,50)\n",
    "np.shape(normalized_featured_data)\n",
    "\n",
    "# Make non-normalized, featured dataset (dataset size (72,6,15))\n",
    "featured_data = features(data,.75,50)\n",
    "\n",
    "# Make dataset that only has max of each feature\n",
    "maxed_data = everything_max(featured_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6 12 18 24 30 36 42 48 54 60 66 "
     ]
    }
   ],
   "source": [
    "# Condense and featurify the normalized data\n",
    "condensed_data = condense(normalized_data,10)\n",
    "condensed_data = features(condensed_data,.75,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 1, 6)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(maxed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels = [3 2 2 2 0 0 1 1 2 2 0 3]\n",
      "Correct labels =   [3 2 2 2 0 0 1 1 2 2 3 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91.66666666666666"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model on normalized and feature-ified data\n",
    "make_model(normalized_featured_data,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels = [3 2 2 2 1 0 1 1 2 2 1 3]\n",
      "Correct labels =   [3 2 2 2 0 0 1 1 2 2 3 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "83.33333333333334"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model on non-normalized, featured data (dataset size (72,6,15))\n",
    "make_model(featured_data,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels = [0 2 2 2 0 0 1 1 2 2 1 3]\n",
      "Correct labels =   [3 2 2 2 0 0 1 1 2 2 3 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "83.33333333333334"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model on max-feature-engineered data\n",
    "make_model(maxed_data,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels = [0 1 1 1 1 2 0 0 0 1 0 2 2 2 0 0 1 0 2 2 0 0]\n",
      "Correct labels =   [1 1 1 1 1 2 3 3 3 3 3 2 2 2 0 0 1 1 2 2 3 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59.09090909090909"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model on condensed normalized data\n",
    "make_model(condensed_data,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare raw vs. condensed data (normalized). Condensed once, factor 40\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "ax1.plot(data[5])\n",
    "ax2.plot(condensed_data[5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tests of each type with high condense factor to see if any features emerge I guess?\n",
    "\n",
    "condensed_data = condense(data,100)\n",
    "fig, ((ax1, ax2),(ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 6))\n",
    "ax1.plot(condensed_data[65])\n",
    "ax2.plot(condensed_data[67])\n",
    "ax3.plot(condensed_data[69])\n",
    "ax4.plot(condensed_data[71])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on max-feature-engineered data. This is eeeeeasily the best model holy crap\n",
    "\n",
    "make_model(maxed_featured_data,59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try try testing different condense factors\n",
    "\n",
    "condense_factor = 10\n",
    "\n",
    "for i in range(num):\n",
    "    condensed_data = condense(data)\n",
    "    featured_data = features(condensed_data, )\n",
    "    maxed_data = everything_max(featured_data)\n",
    "    percent_correct[i] = make_model(maxed_data, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try models with data without accelerometer data. \n",
    "\n",
    "data_no_acc\n",
    "featured_no_acc = features(data_no_acc,.75,30)\n",
    "condensed_no_acc = condense(data_no_acc)\n",
    "max_featured_no_acc = everything_max(featured_no_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on data without accelerometer data. I tried this and there was no change, so might as well not include\n",
    "# the accelerometer data unless I can think of features that differentiate between accelerometer data\n",
    "# Pretty sure having the accelerometer data makes no difference\n",
    "\n",
    "make_model(max_featured_no_acc,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best change factor for change in max function. This is really not an efficient way to do this...\n",
    "\n",
    "#start_time = time()\n",
    "\n",
    "num = 100\n",
    "\n",
    "percent_correct = np.zeros(num)\n",
    "\n",
    "def features2(array, change_in_max_change_factor):\n",
    "    len_dataset, _, number_of_columns = np.shape(array)\n",
    "    number_of_features = 1\n",
    "    condensed_data = np.zeros((len_dataset,number_of_features,number_of_columns))\n",
    "    for j in range(72):\n",
    "        data_point = array[j]\n",
    "        number_of_columns = len(data_point[0,:])\n",
    "        D = np.zeros((number_of_features,number_of_columns),dtype=np.float64)\n",
    "        for i in range(number_of_columns):\n",
    "            column = data_point[:,i]\n",
    "            D[0,i] = change_in_max(column,change_in_max_change_factor)\n",
    "        condensed_data[j]=D\n",
    "        if np.mod(j,6) == 0:\n",
    "            print(j, end=\" \")\n",
    "    return condensed_data\n",
    "\n",
    "for i in range(num):\n",
    "    featured_data = features2(data_no_acc,i) #for speed ?\n",
    "    maxed_data = everything_max(featured_data)\n",
    "    percent_correct[i] = make_model(maxed_data, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(num),percent_correct)\n",
    "plt.show()\n",
    "max(percent_correct) # max is around 50 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best three_quarters value. .6 --> .8?\n",
    "\n",
    "#start_time = time()\n",
    "\n",
    "num = [.6, .61, .62, .63, .64, .65, .66, .67, .68, .69, .7, .71, .72, .73, .74, .75, .76, .77, .78, .79, 8]\n",
    "\n",
    "percent_correct = np.zeros(len(num))\n",
    "\n",
    "def features2(array, three_quarters_percent):\n",
    "    len_dataset, _, number_of_columns = np.shape(array)\n",
    "    number_of_features = 1\n",
    "    condensed_data = np.zeros((len_dataset,number_of_features,number_of_columns))\n",
    "    for j in range(72):\n",
    "        data_point = array[j]\n",
    "        number_of_columns = len(data_point[0,:])\n",
    "        D = np.zeros((number_of_features,number_of_columns),dtype=np.float64)\n",
    "        for i in range(number_of_columns):\n",
    "            column = data_point[:,i]\n",
    "            D[0,i] = three_quarters(column,three_quarters_percent)\n",
    "        condensed_data[j]=D\n",
    "        if np.mod(j,6) == 0:\n",
    "            print(j, end=\" \")\n",
    "    return condensed_data\n",
    "\n",
    "for i in num:\n",
    "    featured_data = features2(data_no_acc,i) #for speed\n",
    "    maxed_data = everything_max(featured_data)\n",
    "    percent_correct[i] = make_model(maxed_data, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is some Tensorflow practice stuff that I'm not currently working on but don't want to lose\n",
    "\n",
    "batch_size = 12\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = (train, train_labels)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((features, labels)).repeat().batch(12)\n",
    "\n",
    "features, labels = (test, test_labels)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((features, labels)).repeat().batch(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = train_dataset.__iter__()\n",
    "test_iter = test_dataset.__iter__()\n",
    "\n",
    "train_x, train_y = train_iter.get_next()\n",
    "test_x, test_y = test_iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test_dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_features_vector(features, labels):\n",
    "    #Pack the features into a single array.\n",
    "    features = tf.stack(list(features, axis=1))\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(pack_features_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make a simple model\n",
    "# net = tf.keras.layers.Dense(train_x, 8) # pass the first value from iter.get_next() as input\n",
    "# net = tf.keras.layers.dense(net, 8)\n",
    "# prediction = tf.layers.dense(net, 1)\n",
    "# loss = tf.losses.mean_squared_error(prediction, train_y) # pass the second value from iter.get_net() as label\n",
    "# train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l0 = tf.keras.layers.Dense(units=1, input_shape=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(units = 112016, input_shape=(None, 7001, 16)),\n",
    "    #tf.keras.layers.Dense(units=112016, input_shape=(None, 7001, 16), activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 12\n",
    "# train = train.repeat().shuffle(62).batch(BATCH_SIZE)\n",
    "# test = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train, epochs=1, steps_per_epoch=math.ceil(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
